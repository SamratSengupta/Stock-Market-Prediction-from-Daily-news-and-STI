{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\softwares\\conda4.8.2-python3.7\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\softwares\\conda4.8.2-python3.7\\lib\\site-packages\\numpy\\.libs\\libopenblas.IPBC74C7KURV7CB2PKT5Z5FNR3SIBV4J.gfortran-win_amd64.dll\n",
      "C:\\softwares\\conda4.8.2-python3.7\\lib\\site-packages\\numpy\\.libs\\libopenblas.noijjg62emaszi6nyurl6jbkm4evbgm7.gfortran-win_amd64.dll\n",
      "C:\\softwares\\conda4.8.2-python3.7\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "C:\\softwares\\conda4.8.2-python3.7\\lib\\site-packages\\numpy\\.libs\\libopenblas.TXA6YQSD3GCQQC22GEQ54J2UDCXDXHWN.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functools import partial\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "from sklearn import preprocessing\n",
    "import category_encoders as ce\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor ##Import Tabnet \n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from pathlib import Path\n",
    "import os.path as osp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import median_absolute_error as mae\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import median_absolute_error as mae\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "from torch.optim import Adam\n",
    "import os.path as osp\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from pytorch_transformers import *\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "from pathlib import Path\n",
    "import os.path as osp\n",
    "from utils import normalize_price\n",
    "from transformers import BertModel\n",
    "from pickle import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataPreparation import clean_text,derive_technical_indicators\n",
    "from BertCapsule import bertCapsuleModel\n",
    "from utils import merge_headlines,normalize_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = osp.join(os.getcwd(),'Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and preprocess dowjones realtime data\n",
    "dj = pd.read_csv(data_path + '/dowjones_realtime.csv')\n",
    "dj = dj.set_index('Date')\n",
    "dj = dj.sort_index(ascending=True)\n",
    "dj['cap'] =  dj['Open'].shift(-1) - dj['Open']\n",
    "dj_sti = derive_technical_indicators(dj)\n",
    "#dj_sti = normalize(dj_sti)\n",
    "dj_sti['Date']=dj_sti.index\n",
    "dj_sti.reset_index(drop=True, inplace=True)\n",
    "dj_sti['Date'] = pd.to_datetime(dj_sti['Date']).dt.strftime('%d-%m-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and preprocess rreddit news data\n",
    "\n",
    "news = pd.read_csv(data_path + '/news_realtime.csv')\n",
    "news['Date'] = pd.to_datetime(news['Date']).dt.strftime('%d-%m-%Y')\n",
    "news['News'] = news['title']\n",
    "news = news.drop(['date','score','Unnamed: 0','title'],axis=1)\n",
    "#all_headlines,dates = merge_headlines(dj_sti,news,max_headline_length = 20,max_daily_length = 400) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge for common date\n",
    "all_headlines,dates = merge_headlines(dj_sti,news,max_headline_length = 20,max_daily_length = 400)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = osp.join(os.getcwd(),'Models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64da398cf2fd482f893ca3cc9217977e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "output shape  torch.Size([1, 1])\n",
      "output shape  torch.Size([1, 1])\n",
      "output shape  torch.Size([1, 1])\n",
      "output shape  torch.Size([1, 1])\n",
      "output shape  torch.Size([1, 1])\n",
      "output shape  torch.Size([1, 1])\n",
      "output shape  torch.Size([1, 1])\n",
      "output shape  torch.Size([1, 1])\n",
      "output shape  torch.Size([1, 1])\n",
      "output shape  torch.Size([1, 1])\n",
      "output shape  torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "#load bert capsule model and calculate bert predictions \n",
    "\n",
    "\n",
    "state_dict = torch.load(model_path + '/bert-capsule.pth')\n",
    "#ensemble bert capsule hyper parameters\n",
    "ebc_model = bertCapsuleModel(input_dim_capsule=768,\n",
    "                                     num_capsule=10,dim_capsule=16,\n",
    "                                     routings=5,kernel_size=(9,1),\n",
    "                                     dropout_p=0.25,T_epsilon = 1e-7,\n",
    "                                     batch_size=128)\n",
    "ebc_model.state_dict = state_dict\n",
    "\n",
    "ebc_scaler = load(open(model_path + '/bert-capsule-scaler.pkl', 'rb'))\n",
    "norm_price = ebc_scaler.transform(dj_sti['cap'].to_numpy().reshape(-1,1))\n",
    "\n",
    "\n",
    "MAX_LEN = 400\n",
    "batch_size = 4\n",
    "input_ids=[]\n",
    "\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in all_headlines]\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "for i in tqdm_notebook(range(len(tokenized_texts))):\n",
    "       input_ids.append(tokenizer.convert_tokens_to_ids(tokenized_texts[i]))        \n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "#Create attention masks\n",
    "attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:    \n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "    \n",
    "test_inputs,test_labels = input_ids, norm_price\n",
    "test_masks, _ = attention_masks, input_ids\n",
    "test_inputs =  torch.from_numpy(np.array(test_inputs)).long()\n",
    "test_labels = torch.from_numpy(np.array(test_labels)).float()\n",
    "test_masks = torch.from_numpy(np.array(test_masks)).float()\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=1)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    ebc_model = ebc_model.to(device) \n",
    "    \n",
    "\n",
    "predictions = []\n",
    "test_labels = []\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, batch in enumerate(test_dataloader):\n",
    "        \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "      # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "      # Forward pass\n",
    "        outputs = ebc_model.forward(b_input_ids,b_input_mask)\n",
    "      # print (outputs)target.\n",
    "        test_label = b_labels.cpu().data.numpy().tolist()\n",
    "        test_labels.append(test_label)\n",
    "        prediction = outputs.cpu().data.numpy().flatten().tolist()   \n",
    "        predictions.append(prediction)\n",
    "        \n",
    "preds = ebc_scaler.inverse_transform(predictions)[:, [0]].flatten()\n",
    "preds = preds.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load tabnet model and calculate tabnet predictions \n",
    "\n",
    "tbr_scaler = load(open(model_path + '/tabnet-scaler.pkl', 'rb'))\n",
    "tbnt_regr = torch.load(model_path + '/tabnet.pt')\n",
    "\n",
    "dj_sti=dj_sti.drop(['Date'],axis=1)\n",
    "scaled_features = tbr_scaler.transform(dj_sti.values)\n",
    "test = pd.DataFrame(scaled_features, index=dj_sti.index, columns=dj_sti.columns)\n",
    "\n",
    "target = 'cap'\n",
    "test_indices = test.index\n",
    "for col in test.columns[test.dtypes == 'float64']:\n",
    "    test.fillna(test.loc[test_indices, col].mean(), inplace=True)\n",
    "\n",
    "unused_feat = ['Set']\n",
    "features = [ col for col in test.columns if col not in unused_feat+[target]] \n",
    "\n",
    "X_test = test[features].values[test_indices]\n",
    "y_test = test[target].values[test_indices].reshape(-1, 1)\n",
    "\n",
    "tb_predictions = tbnt_regr.predict(X_test)\n",
    "tb_preds = ebc_scaler.inverse_transform(tb_predictions)[:, [0]].flatten()\n",
    "tb_preds = tb_preds.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = pd.DataFrame.from_dict({'date':dates, 'cap' : dj_sti['cap'],'ebc_pred' : preds ,'tb_pred':tb_preds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the ensemble coefficient for bert and tabnet were  0.497 and 0.0698 with intercept as 526.204"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df['predicted cap'] = 0.497 * main_df['ebc_pred'] + 0.0418 * main_df['tb_pred'] + 896.216"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = main_df.drop(['ebc_pred','tb_pred'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "540.3392571952049"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae(main_df['cap'].values,main_df['predicted cap'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "602.950010809516"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(main_df['cap'].values,main_df['predicted cap'].values,squared=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
